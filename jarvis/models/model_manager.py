"""
Model Manager for Jarvis.
This module handles model selection and interaction.
"""
import logging
from typing import Dict, Any, Optional, List

from .openai_model import OpenAIModel
from .local_model import OllamaModel

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelManager:
    """
    Manager for handling model selection and interaction.
    Uses OpenAI GPT-4o-mini by default, with local Ollama LLM as fallback.
    """
    
    def __init__(self):
        """Initialize the model manager."""
        # Try to initialize OpenAI model (primary)
        self.openai_available = False
        try:
            self.openai_model = OpenAIModel()
            self.openai_available = True
            logger.info("‚úÖ OpenAI GPT-4o-mini initialized (primary)")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è OpenAI model initialization failed: {str(e)}")
            self.openai_model = None
            
        # Try to initialize Ollama model as fallback
        self.ollama_available = False
        try:
            self.ollama_model = OllamaModel()
            # Quick check if Ollama is actually available
            if self.ollama_model.is_available():
                self.ollama_available = True
                logger.info(f"‚úÖ Ollama {self.ollama_model.model_name} initialized (fallback)")
            else:
                logger.warning("‚ö†Ô∏è Ollama initialized but not responding")
                self.ollama_model = None
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ollama model initialization failed: {str(e)}")
            self.ollama_model = None
    
    def generate(self, prompt: str, system_prompt: Optional[str] = None, 
                 temperature: float = 0.7, max_tokens: int = 1000) -> str:
        """
        Generate a response using the available model.
        
        Args:
            prompt: The user's query
            system_prompt: Optional system instructions
            temperature: Controls randomness (0-1)
            max_tokens: Maximum tokens to generate
            
        Returns:
            Generated response
        """
        # Try OpenAI GPT-4o-mini first (primary)
        if self.openai_available:
            try:
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": prompt})
                
                response = self.openai_model.generate_response(messages, temperature, max_tokens)
                logger.debug("‚úÖ Response generated by GPT-4o-mini")
                return response
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è GPT-4o-mini failed: {str(e)}")
                logger.info("üîÑ Falling back to local Ollama model...")
                
        # Fall back to Ollama if OpenAI fails
        if self.ollama_available:
            try:
                response = self.ollama_model.generate(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                # Check if Ollama returned an error
                if not response.startswith("Error:"):
                    logger.info("‚úÖ Response generated by Ollama (fallback)")
                    return response
                else:
                    logger.error(f"‚ùå Ollama failed: {response}")
                    return "Error: Both GPT-4o-mini and local Ollama model failed to generate a response."
                    
            except Exception as e:
                logger.error(f"‚ùå Ollama model failed: {str(e)}")
                return "Error: Both GPT-4o-mini and local Ollama model failed to generate a response."
                
        return "Error: No models available. Please configure OPENAI_KEY or ensure Ollama is running."
    
    def generate_response(self, messages: List[Dict[str, str]], 
                         temperature: float = 0.7, max_tokens: int = 1000) -> str:
        """
        Generate a response using the available model with message format.
        
        Args:
            messages: List of message dictionaries with 'role' and 'content'
            temperature: Controls randomness (0-1)
            max_tokens: Maximum tokens to generate
            
        Returns:
            Generated response
        """
        # Try OpenAI GPT-4o-mini first (primary)
        if self.openai_available:
            try:
                response = self.openai_model.generate_response(messages, temperature, max_tokens)
                logger.debug("‚úÖ Response generated by GPT-4o-mini")
                return response
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è GPT-4o-mini failed: {str(e)}")
                logger.info("üîÑ Falling back to local Ollama model...")
                
        # Fall back to Ollama if OpenAI fails
        if self.ollama_available:
            try:
                # Convert messages to prompt format for Ollama
                prompt = ""
                for msg in messages:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    if role == "system":
                        prompt += f"System: {content}\n\n"
                    elif role == "user":
                        prompt += f"User: {content}\n\n"
                    elif role == "assistant":
                        prompt += f"Assistant: {content}\n\n"
                
                response = self.ollama_model.generate(
                    prompt=prompt.strip(),
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                # Check if Ollama returned an error
                if not response.startswith("Error:"):
                    logger.info("‚úÖ Response generated by Ollama (fallback)")
                    return response
                else:
                    logger.error(f"‚ùå Ollama failed: {response}")
                    return "Error: Both GPT-4o-mini and local Ollama model failed to generate a response."
                    
            except Exception as e:
                logger.error(f"‚ùå Ollama model failed: {str(e)}")
                return "Error: Both GPT-4o-mini and local Ollama model failed to generate a response."
                
        return "Error: No models available. Please configure OPENAI_KEY or ensure Ollama is running."

def get_model():
    """Get a model instance for text generation.
    
    Returns:
        A model instance that can generate text responses
    """
    manager = ModelManager()
    
    # Return the first available model
    if manager.openai_available:
        return manager.openai_model
    elif manager.ollama_available:
        return manager.ollama_model
    else:
        raise RuntimeError("No models available. Please configure OPENAI_KEY or ensure Ollama is running.") 